#![doc = include_str!("readme.md")]

use crate::{
    reader::{SourcePosition, Token, TokenStream},
    GaiaDiagnostics, GaiaError, SourceLocation,
};
use url::Url;

/// è¡¨ç¤ºä»¤ç‰Œç±»å‹ã€‚
pub trait TokenType: Copy {
    /// è¡¨ç¤ºæµçš„ç»“æŸã€‚
    const END_OF_STREAM: Self;

    /// æ£€æŸ¥ä»¤ç‰Œæ˜¯å¦ä¸ºç©ºæ ¼ã€‚
    fn is_whitespace(&self) -> bool {
        false
    }

    /// æ£€æŸ¥ä»¤ç‰Œæ˜¯å¦åº”è¢«å¿½ç•¥ã€‚
    fn is_ignored(&self) -> bool {
        false
    }
}

/// è¯æ³•åˆ†æå™¨çŠ¶æ€ç®¡ç†å®ç”¨ç±»
///
/// è¿™æ˜¯ä¸€ä¸ªé€šç”¨çš„è¯æ³•åˆ†æå™¨çŠ¶æ€ç®¡ç†å™¨ï¼Œæä¾›äº†å®Œæ•´çš„è¯æ³•åˆ†æåŠŸèƒ½ï¼Œ
/// åŒ…æ‹¬å­—ç¬¦ä½ç½®è·Ÿè¸ªã€token æ”¶é›†ã€é”™è¯¯å¤„ç†ç­‰ã€‚
///
/// # è®¾è®¡ç›®æ ‡
///
/// * **é€šç”¨æ€§**: æ”¯æŒä»»æ„ token ç±»å‹ï¼Œåªè¦å®ç° `Copy` trait
/// * **æ€§èƒ½**: é«˜æ•ˆçš„å­—ç¬¦è¿­ä»£å’Œä½ç½®è·Ÿè¸ª
/// * **æ˜“ç”¨æ€§**: æä¾›ä¸°å¯Œçš„è¾…åŠ©æ–¹æ³•ç®€åŒ–è¯æ³•åˆ†æ
/// * **é”™è¯¯å¤„ç†**: é›†æˆ Gaia é”™è¯¯ç³»ç»Ÿ
///
/// # ç¤ºä¾‹
///
/// ```rust
/// use gaia_types::{
///     lexer::LexerState,
///     reader::{SourcePosition, Token},
/// };
///
/// #[derive(Clone, Copy, Debug)]
/// enum MyToken {
///     Identifier,
///     Number,
///     Whitespace,
/// }
///
/// let input = "hello 123";
/// let mut state = LexerState::new(input);
///
/// // æ·»åŠ  token
/// state.add_token(MyToken::Identifier, 0, 5, 1, 1);
/// state.add_token(MyToken::Whitespace, 5, 1, 1, 6);
/// state.add_token(MyToken::Number, 6, 3, 1, 7);
///
/// // ç”Ÿæˆ token æµ
/// let token_stream = state.into_token_stream();
/// ```
#[derive(Debug)]
pub struct LexerState<'input, T: TokenType> {
    url: Option<&'input Url>,
    /// è¾“å…¥å­—ç¬¦ä¸²
    input: &'input str,
    /// æ”¶é›†çš„ tokens
    tokens: Vec<Token<T>>,
    /// å½“å‰è¡Œå·ï¼ˆä» 1 å¼€å§‹ï¼‰
    line: u32,
    /// å½“å‰åˆ—å·ï¼ˆä» 1 å¼€å§‹ï¼ŒUTF-16 é•¿åº¦ï¼‰
    column: u32,
    /// å½“å‰å­—èŠ‚åç§»é‡ï¼ˆä» 0 å¼€å§‹ï¼‰
    offset: usize,
    diagnostics: Vec<GaiaError>,
}

impl<'input, T: TokenType> LexerState<'input, T> {
    /// åˆ›å»ºæ–°çš„è¯æ³•åˆ†æå™¨çŠ¶æ€
    ///
    /// # å‚æ•°
    ///
    /// * `input` - è¦åˆ†æçš„è¾“å…¥å­—ç¬¦ä¸²
    /// * `url` - è¾“å…¥æºçš„ URLï¼ˆå¯é€‰ï¼‰ï¼Œç”¨äºé”™è¯¯å®šä½
    ///
    /// # è¿”å›å€¼
    ///
    /// è¿”å›åˆå§‹åŒ–çš„è¯æ³•åˆ†æå™¨çŠ¶æ€ï¼Œä½ç½®ä¿¡æ¯è®¾ç½®ä¸º (line: 1, column: 1, offset: 0)
    ///
    /// # ç¤ºä¾‹
    ///
    /// ```rust
    /// use gaia_types::lexer::LexerState;
    /// use url::Url;
    ///
    /// let input = "let x = 42;";
    /// let url = Url::parse("file:///example.gs").ok();
    /// let state = LexerState::new(input, url);
    /// assert_eq!(state.get_position(), (0, 1, 1));
    /// ```
    pub fn new(input: &'input str, url: Option<&'input Url>) -> Self {
        Self { url, input, tokens: Vec::new(), line: 1, column: 1, offset: 0, diagnostics: vec![] }
    }

    /// è·å–å½“å‰ä½ç½®ä¿¡æ¯
    ///
    /// è¿”å›å½“å‰çš„å­—èŠ‚åç§»é‡ã€è¡Œå·å’Œåˆ—å·
    ///
    /// # è¿”å›å€¼
    ///
    /// è¿”å› `(offset, line, column)` å…ƒç»„/// è·å–å½“å‰ä½ç½®ï¼ˆè¡Œã€åˆ—ã€åç§»é‡ï¼‰
    ///
    /// # è¿”å›å€¼
    ///
    /// è¿”å›ä¸€ä¸ªä¸‰å…ƒç»„ `(offset, line, column)`ï¼Œå…¶ä¸­ï¼š
    /// * `offset` - å­—èŠ‚åç§»é‡ï¼ˆä» 0 å¼€å§‹ï¼‰
    /// * `line` - è¡Œå·ï¼ˆä» 1 å¼€å§‹ï¼‰
    /// * `column` - åˆ—å·ï¼ˆä» 1 å¼€å§‹ï¼ŒUTF-16 é•¿åº¦ï¼‰
    ///
    /// # ç¤ºä¾‹
    ///
    /// ```rust
    /// let input = "hello\nworld";
    /// let mut state = LexerState::new(input);
    ///
    /// // åˆå§‹ä½ç½®
    /// assert_eq!(state.get_position(), (0, 1, 1));
    ///
    /// // ç§»åŠ¨åˆ° 'o' å­—ç¬¦
    /// state.next_char(); // h
    /// state.next_char(); // e
    /// state.next_char(); // l
    /// state.next_char(); // l
    /// state.next_char(); // o
    /// assert_eq!(state.get_position(), (4, 1, 5)); // "hello" çš„æœ€åä¸€ä¸ªå­—ç¬¦
    /// ```
    pub fn get_position(&self) -> (usize, u32, u32) {
        (self.offset, self.line, self.column)
    }

    /// æ ‡è®°å½“å‰ä½ç½®å¹¶è¿”å›ä½ç½®ä¿¡æ¯
    ///
    /// è¿™æ˜¯ä¸€ä¸ªä¾¿åˆ©æ–¹æ³•ï¼Œç”¨äºåœ¨éœ€è¦è®°å½• token å¼€å§‹ä½ç½®æ—¶ä½¿ç”¨ã€‚
    /// è¿”å›å½“å‰çš„å­—èŠ‚åç§»é‡ã€è¡Œå·å’Œåˆ—å·ï¼Œé€šå¸¸ç”¨äºåç»­çš„ `add_token` è°ƒç”¨ã€‚
    ///
    /// # è¿”å›å€¼
    ///
    /// `(offset, line, column)` - å½“å‰ä½ç½®çš„ä¸‰å…ƒç»„
    ///
    /// # ç¤ºä¾‹
    ///
    /// ```rust
    /// let input = "hello";
    /// let mut state = LexerState::new(input, None);
    ///
    /// let (start_offset, start_line, start_column) = state.mark_position();
    /// // å¤„ç†ä¸€äº›å­—ç¬¦...
    /// state.add_token(SomeTokenType::Identifier, start_offset, 5, start_line, start_column);
    /// ```
    pub fn mark_position(&self) -> (usize, u32, u32) {
        (self.offset, self.line, self.column)
    }

    /// è·å–å½“å‰æºä»£ç ä½ç½®
    ///
    /// è¿”å›ä¸€ä¸ª `SourcePosition` ç»“æ„ä½“ï¼ŒåŒ…å«å®Œæ•´çš„ä½ç½®ä¿¡æ¯
    pub fn get_source_position(&self) -> SourcePosition {
        SourcePosition { offset: self.offset, length: 0, line: self.line, column: self.column }
    }

    /// æŸ¥çœ‹ä¸‹ä¸€ä¸ªå­—ç¬¦ä½†ä¸æ¶ˆè€—å®ƒ
    ///
    /// è¿”å›ä¸‹ä¸€ä¸ªå­—ç¬¦çš„å‰¯æœ¬åŠå…¶å­—èŠ‚åç§»é‡ï¼Œä½†ä¸ç§»åŠ¨ä½ç½®æŒ‡é’ˆã€‚å¯ä»¥å¤šæ¬¡è°ƒç”¨ `peek` è·å–ç›¸åŒçš„å­—ç¬¦ã€‚
    ///
    /// # è¿”å›å€¼
    ///
    /// * `Some((offset, ch))` - ä¸‹ä¸€ä¸ªå­—ç¬¦ `ch` åŠå…¶å­—èŠ‚åç§»é‡
    /// * `None` - å·²åˆ°è¾¾è¾“å…¥æµæœ«å°¾
    ///
    /// # ç¤ºä¾‹
    ///
    /// ```rust
    /// let input = "hello";
    /// let mut state = LexerState::new(input);
    ///
    /// // å¤šæ¬¡ peek è¿”å›ç›¸åŒçš„å­—ç¬¦
    /// assert_eq!(state.peek(), Some((0, 'h')));
    /// assert_eq!(state.peek(), Some((0, 'h')));
    /// assert_eq!(state.peek(), Some((0, 'h')));
    ///
    /// // ä½ç½®ä¿¡æ¯ä¸å˜
    /// assert_eq!(state.get_position(), (0, 1, 1));
    ///
    /// // next_char ä¼šæ¶ˆè€—å­—ç¬¦å¹¶ç§»åŠ¨ä½ç½®
    /// assert_eq!(state.next_char(), Some((0, 'h')));
    /// assert_eq!(state.peek(), Some((1, 'e')));
    /// assert_eq!(state.get_position(), (1, 1, 2));
    /// ```
    pub fn peek(&mut self) -> Option<(usize, char)> {
        let char = self.rest_text().chars().next()?;
        Some((self.offset, char))
    }

    /// è¿”å›å‰©ä½™çš„æ–‡æœ¬ã€‚
    pub fn rest_text(&self) -> &'input str {
        unsafe {
            debug_assert!(self.offset <= self.input.len());
            self.input.get_unchecked(self.offset..)
        }
    }

    /// è·å–ä¸‹ä¸€ä¸ªå­—ç¬¦å¹¶æ›´æ–°ä½ç½®ä¿¡æ¯
    ///
    /// ä»è¾“å…¥æµä¸­è¯»å–ä¸‹ä¸€ä¸ªå­—ç¬¦ï¼Œå¹¶è‡ªåŠ¨æ›´æ–°è¡Œå·ã€åˆ—å·å’Œåç§»é‡ã€‚
    /// åˆ—å·è®¡ç®—ä½¿ç”¨ UTF-16 é•¿åº¦ï¼Œä¸ LSP åè®®å…¼å®¹ã€‚
    ///
    /// # è¿”å›å€¼
    ///
    /// * `Some((offset, ch))` - æˆåŠŸè¯»å–å­—ç¬¦ `ch` åŠå…¶å­—èŠ‚åç§»é‡
    /// * `None` - å·²åˆ°è¾¾è¾“å…¥æµæœ«å°¾
    ///
    /// # ç¤ºä¾‹
    ///
    /// ```rust
    /// let input = "a\nğŸ˜€";
    /// let mut state = LexerState::new(input);
    ///
    /// // è¯»å– 'a'
    /// assert_eq!(state.next_char(), Some((0, 'a')));
    /// assert_eq!(state.get_position(), (1, 1, 2)); // UTF-16 é•¿åº¦: 'a' = 1
    ///
    /// // è¯»å–æ¢è¡Œç¬¦
    /// assert_eq!(state.next_char(), Some((1, '\n')));
    /// assert_eq!(state.get_position(), (2, 2, 1)); // æ–°è¡Œå¼€å§‹
    ///
    /// // è¯»å–è¡¨æƒ…ç¬¦å·ï¼ˆä»£ç†å¯¹ï¼ŒUTF-16 é•¿åº¦ = 2ï¼‰
    /// assert_eq!(state.next_char(), Some((2, 'ğŸ˜€')));
    /// assert_eq!(state.get_position(), (6, 2, 3)); // UTF-16 é•¿åº¦: 'ğŸ˜€' = 2ï¼Œæ‰€ä»¥ column = 1 + 2 = 3
    /// ```
    pub fn next_char(&mut self) -> Option<(usize, char)> {
        let (_, this_char) = self.peek()?;
        self.consume_char(this_char);
        self.peek()
    }

    /// æ›´æ–°ä½ç½®ä¿¡æ¯ï¼ˆå¤„ç†æ¢è¡Œå’Œ Unicodeï¼‰
    ///
    /// æ­£ç¡®å¤„ç†å„ç§æ¢è¡Œç¬¦ï¼ˆ\n, \r, \r\nï¼‰å’Œ Unicode å­—ç¬¦ã€‚
    /// åˆ—å·è®¡ç®—ä½¿ç”¨ UTF-16 é•¿åº¦ï¼Œä¸ LSP åè®®å…¼å®¹ã€‚
    ///
    /// # UTF-16 ç¼–ç è§„åˆ™
    ///
    /// * åŸºæœ¬å¤šè¯­è¨€å¹³é¢ (BMP) å­—ç¬¦ï¼ˆU+0000 åˆ° U+FFFFï¼‰ï¼š1 ä¸ª UTF-16 ç å…ƒ
    /// * è¾…åŠ©å¹³é¢å­—ç¬¦ï¼ˆU+10000 åˆ° U+10FFFFï¼‰ï¼š2 ä¸ª UTF-16 ç å…ƒï¼ˆä»£ç†å¯¹ï¼‰
    ///
    /// # ç¤ºä¾‹
    ///
    /// ```rust
    /// // åŸºæœ¬æ‹‰ä¸å­—ç¬¦ï¼šæ¯ä¸ªå­—ç¬¦ 1 ä¸ª UTF-16 ç å…ƒ
    /// "abc" -> åˆ—å·: 1, 2, 3
    ///
    /// // ä¸­æ–‡å­—ç¬¦ï¼šæ¯ä¸ªå­—ç¬¦ 1 ä¸ª UTF-16 ç å…ƒï¼ˆåœ¨ BMP èŒƒå›´å†…ï¼‰
    /// "ä½ å¥½" -> åˆ—å·: 1, 2
    ///
    /// // è¡¨æƒ…ç¬¦å·ï¼šæ¯ä¸ªå­—ç¬¦ 2 ä¸ª UTF-16 ç å…ƒï¼ˆä»£ç†å¯¹ï¼‰
    /// "ğŸ˜€" -> åˆ—å·: 1, 3ï¼ˆè·³è¿‡ 2ï¼Œå› ä¸ºä»£ç†å¯¹å ç”¨ 2 ä¸ªç å…ƒï¼‰
    /// ```
    ///
    /// # æµ‹è¯•
    ///
    /// ```rust
    /// # use gaia_types::lexer::{LexerState, TokenType};
    /// # #[derive(Clone, Copy, Debug)] enum TestToken { Char, Eof }
    /// # impl TokenType for TestToken { const EOF: Self = TestToken::Eof; }
    /// let mut state = LexerState::<TestToken>::new("ğŸ˜€", None);
    /// assert_eq!(state.get_position(), (0, 1, 1)); // åˆå§‹ä½ç½®
    /// state.next_char(); // è¯»å–è¡¨æƒ…ç¬¦å·
    /// assert_eq!(state.get_position(), (4, 1, 3)); // UTF-16é•¿åº¦=2ï¼Œæ‰€ä»¥column=1+2=3
    /// ```
    /// æ¶ˆè€—å•ä¸ªå­—ç¬¦å¹¶æ›´æ–°ä½ç½®ä¿¡æ¯
    ///
    /// è¿™æ˜¯ä¸€ä¸ªå†…éƒ¨æ–¹æ³•ï¼Œç”¨äºæ¶ˆè€—å•ä¸ªå­—ç¬¦å¹¶æ­£ç¡®æ›´æ–°è¡Œå·å’Œåˆ—å·ã€‚
    /// å®ƒä¼šæ­£ç¡®å¤„ç†æ¢è¡Œç¬¦å’Œ UTF-16 åˆ—è®¡ç®—ã€‚
    ///
    /// # å‚æ•°
    ///
    /// * `ch` - è¦æ¶ˆè€—çš„å­—ç¬¦
    ///
    /// # é‡è¦è¯´æ˜
    ///
    /// **æ­¤æ–¹æ³•ä»…å¤„ç†å•ä¸ªå­—ç¬¦**ï¼Œä¸é€‚ç”¨äºå¤šå­—ç¬¦åºåˆ—å¦‚ `\r\n`ï¼š
    /// * å¯¹äº `\r\n` æ¢è¡Œç¬¦ï¼Œè¯·ä½¿ç”¨ `consume_str` æ–¹æ³•
    /// * æ­¤æ–¹æ³•ä¼šå°† `\r` å’Œ `\n` åˆ†åˆ«ä½œä¸ºç‹¬ç«‹çš„å­—ç¬¦å¤„ç†ï¼Œå¯èƒ½å¯¼è‡´è¡Œå·è®¡ç®—é”™è¯¯
    ///
    /// # ç¤ºä¾‹
    ///
    /// ```rust
    /// let input = "hello\nworld";
    /// let mut state = LexerState::new(input, None);
    ///
    /// // å¤„ç†æ¢è¡Œç¬¦
    /// state.consume_char('\n');
    /// assert_eq!(state.get_position(), (6, 2, 1)); // è¡Œå·å¢åŠ ï¼Œåˆ—å·é‡ç½®
    ///
    /// // å¤„ç†æ™®é€šå­—ç¬¦
    /// state.consume_char('w');
    /// assert_eq!(state.get_position(), (7, 2, 2)); // åˆ—å·å¢åŠ 
    /// ```
    ///
    /// # æ³¨æ„
    ///
    /// å¯¹äºæ¢è¡Œç¬¦å¤„ç†ï¼š
    /// * `\n` - æ­£ç¡®å¢åŠ è¡Œå·ï¼Œåˆ—å·é‡ç½®ä¸º 1
    /// * `\r` - å•ç‹¬å¢åŠ è¡Œå·ï¼Œåˆ—å·é‡ç½®ä¸º 1ï¼ˆä¸æ¨èï¼Œå¯èƒ½å¯¼è‡´é—®é¢˜ï¼‰
    /// * `\r\n` - **ä¸æ”¯æŒ**ï¼Œåº”ä½¿ç”¨ `consume_str(state, "\r\n")`
    fn consume_char(&mut self, ch: char) -> usize {
        if ch == '\n' {
            self.line += 1;
            self.column = 1;
        }
        else if ch == '\r' {
            self.line += 1;
            self.column = 1;
        }
        else {
            // è®¡ç®— UTF-16 é•¿åº¦ï¼Œä¸ LSP åè®®å…¼å®¹
            let utf16_len = ch.len_utf16();
            self.column += utf16_len as u32;
        }
        self.offset += ch.len_utf8();
        ch.len_utf8()
    }

    /// æ¶ˆè€—æŒ‡å®šå­—ç¬¦ä¸²å¹¶æ›´æ–°ä½ç½®ä¿¡æ¯
    ///
    /// è¿™æ˜¯ä¸€ä¸ªå†…éƒ¨æ–¹æ³•ï¼Œç”¨äºæ¶ˆè€—å­—ç¬¦ä¸²å¹¶æ­£ç¡®æ›´æ–°è¡Œå·å’Œåˆ—å·ã€‚
    /// å®ƒä¼šæ­£ç¡®å¤„ç†å„ç§æ¢è¡Œç¬¦ï¼ˆåŒ…æ‹¬ `\r\n`ï¼‰å’Œ UTF-16 åˆ—è®¡ç®—ã€‚
    ///
    /// # å‚æ•°
    ///
    /// * `expected` - è¦æ¶ˆè€—çš„å­—ç¬¦ä¸²
    ///
    /// # è¿”å›å€¼
    ///
    /// è¿”å›æ¶ˆè€—çš„å­—èŠ‚é•¿åº¦
    ///
    /// # é‡è¦ç‰¹æ€§
    ///
    /// **æ­£ç¡®å¤„ç† `\r\n` æ¢è¡Œç¬¦**ï¼š
    /// * å¯¹äº `\r\n` åºåˆ—ï¼Œä¼šæ­£ç¡®è¯†åˆ«ä¸ºå•ä¸ªæ¢è¡Œç¬¦
    /// * è¡Œå·åªå¢åŠ ä¸€æ¬¡ï¼Œåˆ—å·æ­£ç¡®é‡ç½®
    /// * è¿™æ˜¯ä¸ `consume_char` æ–¹æ³•çš„ä¸»è¦åŒºåˆ«
    ///
    /// # ç¤ºä¾‹
    ///
    /// ```rust
    /// let input = "hello\r\nworld";
    /// let mut state = LexerState::new(input, None);
    ///
    /// // æ­£ç¡®å¤„ç† \r\n æ¢è¡Œç¬¦
    /// let length = state.consume_str("\r\n");
    /// assert_eq!(length, 2);
    /// assert_eq!(state.get_position(), (2, 2, 1)); // è¡Œå·åªå¢åŠ ä¸€æ¬¡
    /// ```
    fn consume_str(&mut self, expected: &str) -> usize {
        #[cfg(debug_assertions)]
        unsafe {
            let start = self.offset;
            let end = start + expected.len();
            let input = self.input.get_unchecked(start..end);
            assert_eq!(input, expected);
        }

        let mut chars = expected.chars();
        while let Some(c) = chars.next() {
            match c {
                '\n' => {
                    self.line += 1;
                    self.column = 1;
                    self.offset += 1;
                }
                '\r' => match chars.next() {
                    Some('\n') => {
                        self.line += 1;
                        self.column = 1;
                        self.offset += 2;
                    }
                    Some(other) => {
                        self.line += 1;
                        self.column = 1;
                        self.offset += 1 + other.len_utf8();
                    }
                    None => {
                        self.line += 1;
                        self.column = 1;
                        self.offset += 1;
                    }
                },
                _ => {
                    self.column += c.len_utf16() as u32;
                    self.offset += c.len_utf8();
                }
            }
        }
        expected.len()
    }

    /// æ·»åŠ  token åˆ°æ”¶é›†åˆ—è¡¨
    ///
    /// è¿™æ˜¯ä¸€ä¸ªæ ¸å¿ƒæ–¹æ³•ï¼Œç”¨äºå°† token æ·»åŠ åˆ°å†…éƒ¨çš„ token åˆ—è¡¨ä¸­ã€‚
    /// å®ƒä¼šæ ¹æ®æä¾›çš„ä½ç½®ä¿¡æ¯åˆ›å»º `SourcePosition` å’Œ `Token` å¯¹è±¡ã€‚
    ///
    /// # å‚æ•°
    ///
    /// * `token_type` - token ç±»å‹
    /// * `start_offset` - token èµ·å§‹å­—èŠ‚åç§»é‡ï¼ˆç›¸å¯¹äºè¾“å…¥å­—ç¬¦ä¸²çš„å¼€å§‹ï¼‰
    /// * `length` - token é•¿åº¦ï¼ˆå­—èŠ‚ï¼‰
    /// * `start_line` - token èµ·å§‹è¡Œå·ï¼ˆä» 1 å¼€å§‹ï¼‰
    /// * `start_column` - token èµ·å§‹åˆ—å·ï¼ˆä» 1 å¼€å§‹ï¼ŒUTF-16 åˆ—ï¼‰
    ///
    /// # ç¤ºä¾‹
    ///
    /// ```rust,ignore
    /// // å‡è®¾ MyToken æ˜¯å®ç°äº† TokenType çš„ç±»å‹
    /// state.add_token(MyToken::Keyword, 0, 3, 1, 1);   // "let"
    /// state.add_token(MyToken::Identifier, 4, 1, 1, 5); // "x"
    /// ```
    pub fn add_token(&mut self, token_type: T, start_offset: usize, length: usize, start_line: u32, start_column: u32) {
        let position = SourcePosition { offset: start_offset, length, line: start_line, column: start_column };
        self.tokens.push(Token { token_type, position });
    }

    /// ä»å½“å‰ä½ç½®æ·»åŠ  tokenï¼ˆä¾¿æ·æ–¹æ³•ï¼‰
    ///
    /// è¿™ä¸ªæ–¹æ³•ä½¿ç”¨å½“å‰çš„ä½ç½®ä¿¡æ¯è‡ªåŠ¨åˆ›å»º tokenï¼Œæ¯” `add_token` æ›´æ–¹ä¾¿ã€‚
    ///
    /// # å‚æ•°
    ///
    /// * `token_type` - token ç±»å‹
    /// * `start_offset` - token èµ·å§‹å­—èŠ‚åç§»é‡ï¼ˆç›¸å¯¹äºè¾“å…¥å­—ç¬¦ä¸²çš„å¼€å§‹ï¼‰
    /// * `length` - token é•¿åº¦ï¼ˆå­—èŠ‚ï¼‰
    ///
    /// # ç¤ºä¾‹
    ///
    /// ```rust,ignore
    /// // å‡è®¾ MyToken æ˜¯å®ç°äº† TokenType çš„ç±»å‹
    /// state.add_token_at_current(MyToken::Operator, 0, 1); // "+"
    /// ```
    pub fn add_token_at_current(&mut self, token_type: T, start_offset: usize, length: usize) {
        self.add_token(token_type, start_offset, length, self.line, self.column);
    }

    /// æ·»åŠ å•å­—ç¬¦ token å¹¶è‡ªåŠ¨æ¨è¿›ä½ç½®
    ///
    /// è¿™æ˜¯ä¸€ä¸ªä¾¿åˆ©æ–¹æ³•ï¼Œç”¨äºå¤„ç†å•å­—ç¬¦ tokenï¼ˆå¦‚æ‹¬å·ã€è¿ç®—ç¬¦ç­‰ï¼‰ã€‚
    /// å®ƒä¼šè‡ªåŠ¨æ ‡è®°å½“å‰ä½ç½®ï¼Œæ¶ˆè€—æŒ‡å®šå­—ç¬¦ï¼Œå¹¶æ·»åŠ  tokenã€‚
    ///
    /// # é‡è¦è¯´æ˜
    ///
    /// **æ­¤æ–¹æ³•ä»…å¤„ç†å•ä¸ªå­—ç¬¦**ï¼Œä¸é€‚ç”¨äºå¤šå­—ç¬¦åºåˆ—å¦‚ `\r\n`ï¼š
    /// * å¯¹äº `\r\n` æ¢è¡Œç¬¦ï¼Œè¯·ä½¿ç”¨ `advance_by_str` æ–¹æ³•
    /// * æ­¤æ–¹æ³•ä¼šå°† `\r` å’Œ `\n` åˆ†åˆ«ä½œä¸ºç‹¬ç«‹çš„å­—ç¬¦å¤„ç†ï¼Œå¯èƒ½å¯¼è‡´è¡Œå·è®¡ç®—é”™è¯¯
    ///
    /// # å‚æ•°
    ///
    /// * `token_type` - token ç±»å‹
    /// * `expected_char` - æœŸæœ›çš„å­—ç¬¦
    ///
    /// # è¿”å›å€¼
    ///
    /// * `Some(Token<T>)` - æˆåŠŸåˆ›å»ºçš„ token
    /// * `None` - å¦‚æœä¸‹ä¸€ä¸ªå­—ç¬¦ä¸åŒ¹é…æˆ–å·²åˆ°è¾¾è¾“å…¥æœ«å°¾
    ///
    /// # ç¤ºä¾‹
    ///
    /// ```rust
    /// let input = "{hello}";
    /// let mut state = LexerState::new(input, None);
    ///
    /// // å¤„ç†å·¦å¤§æ‹¬å·
    /// if let Some(token) = state.advance_char(MyToken::LeftBrace, '{') {
    ///     println!("Found left brace at position {:?}", token.position);
    /// }
    /// ```
    ///
    /// # æ³¨æ„
    ///
    /// å¯¹äºæ¢è¡Œç¬¦å¤„ç†ï¼š
    /// * `\n` - æ­£ç¡®å¢åŠ è¡Œå·ï¼Œåˆ—å·é‡ç½®ä¸º 1
    /// * `\r` - å•ç‹¬å¢åŠ è¡Œå·ï¼Œåˆ—å·é‡ç½®ä¸º 1ï¼ˆä¸æ¨èï¼Œå¯èƒ½å¯¼è‡´é—®é¢˜ï¼‰
    /// * `\r\n` - **ä¸æ”¯æŒ**ï¼Œåº”ä½¿ç”¨ `advance_by_str(state, "\r\n")`
    pub fn advance_by_char(&mut self, token_type: T, expected: char) {
        let (start, line, column) = self.mark_position();
        let length = self.consume_char(expected);
        self.add_token(token_type, start, length, line, column);
    }

    /// æ·»åŠ å­—ç¬¦ä¸² token å¹¶è‡ªåŠ¨æ¨è¿›ä½ç½®
    ///
    /// è¿™æ˜¯ä¸€ä¸ªä¾¿åˆ©æ–¹æ³•ï¼Œç”¨äºå¤„ç†å¤šå­—ç¬¦ tokenï¼ˆå¦‚è¿ç®—ç¬¦ã€å…³é”®å­—ã€æ¢è¡Œç¬¦ç­‰ï¼‰ã€‚
    /// å®ƒä¼šè‡ªåŠ¨æ ‡è®°å½“å‰ä½ç½®ï¼Œæ¶ˆè€—æŒ‡å®šå­—ç¬¦ä¸²ï¼Œå¹¶æ·»åŠ  tokenã€‚
    ///
    /// # é‡è¦ç‰¹æ€§
    ///
    /// **æ­£ç¡®å¤„ç† `\r\n` æ¢è¡Œç¬¦**ï¼š
    /// * å¯¹äº `\r\n` åºåˆ—ï¼Œä¼šæ­£ç¡®è¯†åˆ«ä¸ºå•ä¸ªæ¢è¡Œç¬¦
    /// * è¡Œå·åªå¢åŠ ä¸€æ¬¡ï¼Œåˆ—å·æ­£ç¡®é‡ç½®
    /// * è¿™æ˜¯ä¸ `advance_by_char` æ–¹æ³•çš„ä¸»è¦åŒºåˆ«
    ///
    /// # å‚æ•°
    ///
    /// * `token_type` - token ç±»å‹
    /// * `expected` - æœŸæœ›çš„å­—ç¬¦ä¸²
    ///
    /// # ç¤ºä¾‹
    ///
    /// ```rust
    /// let input = "hello\r\nworld";
    /// let mut state = LexerState::new(input, None);
    ///
    /// // æ­£ç¡®å¤„ç† \r\n æ¢è¡Œç¬¦
    /// state.advance_by_str(MyToken::Newline, "\r\n");
    /// assert_eq!(state.get_position(), (2, 2, 1)); // è¡Œå·åªå¢åŠ ä¸€æ¬¡
    ///
    /// // å¤„ç†å¤šå­—ç¬¦è¿ç®—ç¬¦
    /// state.advance_by_str(MyToken::Arrow, "=>");
    /// assert_eq!(state.get_position(), (4, 2, 3));
    /// ```
    ///
    /// # æ³¨æ„
    ///
    /// å¯¹äºæ¢è¡Œç¬¦å¤„ç†ï¼š
    /// * `\n` - æ­£ç¡®å¢åŠ è¡Œå·ï¼Œåˆ—å·é‡ç½®ä¸º 1
    /// * `\r` - æ­£ç¡®å¢åŠ è¡Œå·ï¼Œåˆ—å·é‡ç½®ä¸º 1
    /// * `\r\n` - **æ­£ç¡®è¯†åˆ«ä¸ºå•ä¸ªæ¢è¡Œç¬¦**ï¼Œè¡Œå·åªå¢åŠ ä¸€æ¬¡
    pub fn advance_by_str(&mut self, token_type: T, expected: &str) {
        let (start, line, column) = self.mark_position();
        let length = self.consume_str(expected);
        self.add_token(token_type, start, length, line, column);
    }

    /// æ›´æ–°æœ€åä¸€ä¸ª token çš„ç±»å‹
    ///
    /// è¿™æ˜¯ä¸€ä¸ªä¾¿åˆ©æ–¹æ³•ï¼Œç”¨äºåœ¨éœ€è¦æ ¹æ®å†…å®¹é‡æ–°åˆ†ç±» token æ—¶ä½¿ç”¨ã€‚
    ///
    /// # å‚æ•°
    ///
    /// * `new_token_type` - æ–°çš„ token ç±»å‹
    ///
    /// # è¿”å›å€¼
    ///
    /// * `true` - æˆåŠŸæ›´æ–°
    /// * `false` - æ²¡æœ‰ token å¯ä»¥æ›´æ–°
    ///
    /// # ç¤ºä¾‹
    ///
    /// ```rust
    /// # use gaia_types::lexer::{LexerState, TokenType};
    /// # #[derive(Clone, Copy, Debug, PartialEq)] enum TestToken { StringLiteral, TypeDescriptor, Eof }
    /// # impl TokenType for TestToken { const EOF: Self = TestToken::Eof; }
    /// let input = "string";
    /// let mut state = LexerState::new(input, None);
    ///
    /// // å…ˆæ·»åŠ ä¸€ä¸ªå­—ç¬¦ä¸²å­—é¢é‡
    /// state.read_string_literal(TestToken::StringLiteral, '"');
    /// // æ ¹æ®å†…å®¹é‡æ–°åˆ†ç±»ä¸ºç±»å‹æè¿°ç¬¦
    /// state.update_last_token_type(TestToken::TypeDescriptor);
    /// ```
    pub fn update_last_token_type(&mut self, new_token_type: T) -> bool {
        if let Some(last_token) = self.tokens.last_mut() {
            last_token.token_type = new_token_type;
            true
        }
        else {
            false
        }
    }

    /// è·³è¿‡ç©ºç™½å­—ç¬¦
    ///
    /// æ¶ˆè€—å¹¶æ”¶é›†è¿ç»­çš„ç©ºç™½å­—ç¬¦ä½œä¸ºå•ä¸ª whitespace token
    ///
    /// # å‚æ•°
    ///
    /// * `token_type` - whitespace token çš„ç±»å‹
    pub fn skip_whitespace(&mut self, token_type: T) -> Option<Token<T>> {
        if let Some((start_offset, ch)) = self.peek() {
            if ch.is_whitespace() {
                let start_line = self.line;
                let start_column = self.column;
                let mut length = 0;

                // æ¶ˆè€—è¿ç»­çš„ç©ºç™½å­—ç¬¦
                while let Some((_offset, ch)) = self.peek() {
                    if !ch.is_whitespace() {
                        break;
                    }
                    length += ch.len_utf8();
                    self.next_char();
                }

                let position = SourcePosition { offset: start_offset, length, line: start_line, column: start_column };
                let token = Token { token_type, position };
                self.tokens.push(token.clone());
                return Some(token);
            }
        }
        None
    }

    /// è·³è¿‡å•è¡Œæ³¨é‡Š
    ///
    /// å¦‚æœå½“å‰ä½ç½®æ˜¯æ³¨é‡Šå¼€å§‹ï¼Œåˆ™æ¶ˆè€—æ•´ä¸ªæ³¨é‡Š
    ///
    /// # å‚æ•°
    ///
    /// * `comment_type` - æ³¨é‡Š token çš„ç±»å‹
    /// * `start_marker` - æ³¨é‡Šå¼€å§‹æ ‡è®°ï¼ˆå¦‚ "//"ï¼‰
    ///
    /// # è¿”å›å€¼
    ///
    /// å¦‚æœæˆåŠŸè·³è¿‡æ³¨é‡Šåˆ™è¿”å›æ³¨é‡Š tokenï¼Œå¦åˆ™è¿”å› `None`
    pub fn skip_line_comment(&mut self, comment_type: T, start_marker: &str) -> Option<Token<T>> {
        if let Some((start_offset, _)) = self.peek() {
            // æ£€æŸ¥æ˜¯å¦åŒ¹é…æ³¨é‡Šå¼€å§‹æ ‡è®°
            let remaining = &self.input[start_offset..];
            if remaining.starts_with(start_marker) {
                let start_line = self.line;
                let start_column = self.column;
                let mut length = start_marker.len();

                // æ¶ˆè€—æ³¨é‡Šå†…å®¹ç›´åˆ°è¡Œå°¾
                self.next_char(); // è·³è¿‡ç¬¬ä¸€ä¸ªå­—ç¬¦
                if start_marker.len() > 1 {
                    for _ in 1..start_marker.len() {
                        self.next_char();
                    }
                }

                while let Some((_, ch)) = self.peek() {
                    if ch == '\n' || ch == '\r' {
                        break;
                    }
                    length += ch.len_utf8();
                    self.next_char();
                }

                let position = SourcePosition { offset: start_offset, length, line: start_line, column: start_column };
                let token = Token { token_type: comment_type, position };
                self.tokens.push(token.clone());
                return Some(token);
            }
        }
        None
    }

    /// è¯»å–æ ‡è¯†ç¬¦
    ///
    /// è¯»å–è¿ç»­çš„å­—æ¯ã€æ•°å­—ã€ä¸‹åˆ’çº¿ç­‰ä½œä¸ºæ ‡è¯†ç¬¦
    ///
    /// # å‚æ•°
    ///
    /// * `identifier_type` - æ ‡è¯†ç¬¦ token çš„ç±»å‹
    /// * `is_identifier_char` - åˆ¤æ–­å­—ç¬¦æ˜¯å¦å±äºæ ‡è¯†ç¬¦çš„å‡½æ•°
    ///
    /// # è¿”å›å€¼
    ///
    /// å¦‚æœæˆåŠŸè¯»å–æ ‡è¯†ç¬¦åˆ™è¿”å›æ ‡è¯†ç¬¦ tokenï¼Œå¦åˆ™è¿”å› `None`
    pub fn read_identifier<F>(&mut self, identifier_type: T, is_identifier_char: F) -> Option<Token<T>>
    where
        F: Fn(char) -> bool,
    {
        if let Some((start_offset, ch)) = self.peek() {
            if is_identifier_char(ch) {
                let start_line = self.line;
                let start_column = self.column;
                let mut length = ch.len_utf8();

                self.next_char();

                // è¯»å–è¿ç»­çš„æ ‡è¯†ç¬¦å­—ç¬¦
                while let Some((_, ch)) = self.peek() {
                    if !is_identifier_char(ch) {
                        break;
                    }
                    length += ch.len_utf8();
                    self.next_char();
                }

                let position = SourcePosition { offset: start_offset, length, line: start_line, column: start_column };
                let token = Token { token_type: identifier_type, position };
                self.tokens.push(token.clone());
                return Some(token);
            }
        }
        None
    }

    /// è¯»å–å­—ç¬¦ä¸²å­—é¢é‡
    ///
    /// è¯»å–å®Œæ•´çš„å­—ç¬¦ä¸²å­—é¢é‡ï¼ŒåŒ…æ‹¬å¼•å·
    ///
    /// # å‚æ•°
    ///
    /// * `string_type` - å­—ç¬¦ä¸² token çš„ç±»å‹
    /// * `quote_char` - å­—ç¬¦ä¸²å¼•å·å­—ç¬¦ï¼ˆå¦‚ '"' æˆ– '\''ï¼‰
    ///
    /// # è¿”å›å€¼
    ///
    /// å¦‚æœæˆåŠŸè¯»å–å­—ç¬¦ä¸²åˆ™è¿”å›å­—ç¬¦ä¸² tokenï¼Œå¦åˆ™è¿”å› `None`
    pub fn read_string_literal(&mut self, string_type: T, quote_char: char) -> Option<Token<T>> {
        if let Some((start_offset, ch)) = self.peek() {
            if ch == quote_char {
                let start_line = self.line;
                let start_column = self.column;
                let mut length = ch.len_utf8();

                self.next_char(); // è·³è¿‡å¼€å§‹å¼•å·

                // è¯»å–å­—ç¬¦ä¸²å†…å®¹
                while let Some((_, ch)) = self.peek() {
                    if ch == quote_char {
                        length += ch.len_utf8();
                        self.next_char(); // æ¶ˆè€—ç»“æŸå¼•å·
                        break;
                    }
                    if ch == '\\' {
                        // å¤„ç†è½¬ä¹‰å­—ç¬¦
                        length += ch.len_utf8();
                        self.next_char();
                        if let Some((_, escaped_ch)) = self.peek() {
                            length += escaped_ch.len_utf8();
                            self.next_char();
                        }
                    }
                    else {
                        length += ch.len_utf8();
                        self.next_char();
                    }
                }

                let position = SourcePosition { offset: start_offset, length, line: start_line, column: start_column };
                let token = Token { token_type: string_type, position };
                self.tokens.push(token.clone());
                return Some(token);
            }
        }
        None
    }

    /// è¯»å–æ•°å­—å­—é¢é‡
    ///
    /// è¯»å–è¿ç»­çš„æ•°å­—å­—ç¬¦
    ///
    /// # å‚æ•°
    ///
    /// * `number_type` - æ•°å­— token çš„ç±»å‹
    ///
    /// # è¿”å›å€¼
    ///
    /// å¦‚æœæˆåŠŸè¯»å–æ•°å­—åˆ™è¿”å›æ•°å­— tokenï¼Œå¦åˆ™è¿”å› `None`
    pub fn read_integer(&mut self, number_type: T) -> Option<Token<T>> {
        if let Some((start_offset, ch)) = self.peek() {
            if ch.is_ascii_digit() {
                let start_line = self.line;
                let start_column = self.column;
                let mut length = ch.len_utf8();

                self.next_char();

                // è¯»å–è¿ç»­çš„æ•°å­—å­—ç¬¦
                while let Some((_, ch)) = self.peek() {
                    if !ch.is_ascii_digit() && ch != '.' {
                        break;
                    }
                    length += ch.len_utf8();
                    self.next_char();
                }

                let position = SourcePosition { offset: start_offset, length, line: start_line, column: start_column };
                let token = Token { token_type: number_type, position };
                self.tokens.push(token.clone());
                return Some(token);
            }
        }
        None
    }

    /// è·å–å·²æ”¶é›†çš„ token æ•°é‡
    ///
    /// è¿”å›å½“å‰å·²æ”¶é›†çš„ token æ€»æ•°
    ///
    /// # è¿”å›å€¼
    ///
    /// è¿”å› `usize` ç±»å‹çš„ token æ•°é‡
    ///
    /// # ç¤ºä¾‹
    ///
    /// ```rust
    /// # use gaia_types::lexer::{LexerState, TokenType};
    /// # #[derive(Clone, Copy, Debug, PartialEq)] enum TestToken { Keyword, Identifier, Eof }
    /// # impl TokenType for TestToken { const EOF: Self = TestToken::Eof; }
    /// let input = "let x = 42";
    /// let mut state = LexerState::new(input, None);
    ///
    /// assert_eq!(state.token_count(), 0); // åˆå§‹ä¸ºç©º
    ///
    /// // æ·»åŠ ä¸€äº› token
    /// state.add_token(TestToken::Keyword, 0, 3, 1, 1); // "let"
    /// state.add_token(TestToken::Identifier, 4, 1, 1, 5); // "x"
    ///
    /// assert_eq!(state.token_count(), 2);
    /// ```
    pub fn token_count(&self) -> usize {
        self.tokens.len()
    }

    /// è·å–æŒ‡å®šç´¢å¼•çš„ token
    ///
    /// æ ¹æ®ç´¢å¼•è·å–å·²æ”¶é›†çš„ token å¼•ç”¨
    ///
    /// # å‚æ•°
    ///
    /// * `index` - token çš„ç´¢å¼•ä½ç½®ï¼ˆä» 0 å¼€å§‹ï¼‰
    ///
    /// # è¿”å›å€¼
    ///
    /// * `Some(&Token<T>)` - æˆåŠŸè·å–çš„ token å¼•ç”¨
    /// * `None` - ç´¢å¼•è¶…å‡ºèŒƒå›´
    ///
    /// # ç¤ºä¾‹
    ///
    /// ```rust
    /// let input = "let x";
    /// let mut state = LexerState::new(input, None);
    ///
    /// state.add_token(MyToken::Keyword, 0, 3, 1, 1); // "let"
    /// state.add_token(MyToken::Identifier, 4, 1, 1, 5); // "x"
    ///
    /// if let Some(token) = state.get_token(0) {
    ///     println!("ç¬¬ä¸€ä¸ª token: {:?}", token.token_type);
    /// }
    ///
    /// assert!(state.get_token(2).is_none()); // è¶…å‡ºèŒƒå›´
    /// ```
    pub fn get_token(&self, index: usize) -> Option<&Token<T>> {
        self.tokens.get(index)
    }

    /// æ ‡è®°è¯­æ³•é”™è¯¯
    ///
    /// åœ¨å½“å‰ä½ç½®åˆ›å»ºä¸€ä¸ªè¯­æ³•é”™è¯¯å¹¶æ·»åŠ åˆ°è¯Šæ–­ä¿¡æ¯ä¸­
    ///
    /// # å‚æ•°
    ///
    /// * `message` - é”™è¯¯ä¿¡æ¯ï¼Œå¯ä»¥æ˜¯ä»»ä½•å®ç°äº† `ToString` çš„ç±»å‹
    ///
    /// # ç¤ºä¾‹
    ///
    /// ```rust
    /// # use gaia_types::lexer::{LexerState, TokenType};
    /// # use gaia_types::errors::{GaiaError, SourceLocation};
    /// # #[derive(Clone, Copy, Debug, PartialEq)] enum TestToken { Eof }
    /// # impl TokenType for TestToken { const EOF: Self = TestToken::Eof; }
    /// let input = "let x = @";
    /// let mut state = LexerState::new(input, None);
    ///
    /// // ç§»åŠ¨åˆ°éæ³•å­—ç¬¦ä½ç½®
    /// state.next_char();
    /// state.next_char();
    /// state.next_char();
    /// state.next_char();
    /// state.next_char();
    /// state.next_char();
    /// state.next_char();
    ///
    /// // æ ‡è®°é”™è¯¯
    /// state.mark_error("æ„å¤–çš„å­—ç¬¦ '@'");
    ///
    /// // è·å–è¯Šæ–­ç»“æœæ—¶ä¼šåŒ…å«è¿™ä¸ªé”™è¯¯
    /// let diagnostics = state.success(TestToken::Eof);
    /// assert!(!diagnostics.diagnostics.is_empty());
    /// ```
    pub fn mark_error(&mut self, message: impl ToString) {
        let location = SourceLocation { line: self.line, column: self.column, url: self.url.cloned() };
        let error = GaiaError::syntax_error(message, location);
        self.diagnostics.push(error);
    }

    /// åˆ›å»ºæˆåŠŸçš„è¯Šæ–­ç»“æœ
    ///
    /// åˆ›å»ºåŒ…å« token æµçš„æˆåŠŸè¯Šæ–­ç»“æœï¼Œå¹¶è‡ªåŠ¨æ·»åŠ  EOF token
    ///
    /// è¿™ä¸ªæ–¹æ³•ä¼šæ¶ˆè€— `LexerState`ï¼Œè¿”å›å®Œæ•´çš„è¯æ³•åˆ†æç»“æœã€‚
    /// å®ƒä¼šè‡ªåŠ¨åœ¨ token æµæœ«å°¾æ·»åŠ  EOF tokenï¼Œè¡¨ç¤ºè¾“å…¥ç»“æŸã€‚
    ///
    /// # å‚æ•°
    ///
    /// * `eof_type` - EOF token çš„ç±»å‹
    ///
    /// # è¿”å›å€¼
    ///
    /// è¿”å› `GaiaDiagnostics<TokenStream<T>>` å®ä¾‹ï¼ŒåŒ…å«ï¼š
    /// * `Ok(TokenStream<T>)` - æˆåŠŸçš„ token æµ
    /// * `diagnostics` - æ”¶é›†åˆ°çš„æ‰€æœ‰è¯Šæ–­ä¿¡æ¯ï¼ˆè­¦å‘Šã€éè‡´å‘½é”™è¯¯ï¼‰
    ///
    /// # ç¤ºä¾‹
    ///
    /// ```rust
    /// # use gaia_types::lexer::{LexerState, TokenType};
    /// # #[derive(Clone, Copy, Debug, PartialEq)] enum TestToken { Keyword, Identifier, Eof }
    /// # impl TokenType for TestToken { const EOF: Self = TestToken::Eof; }
    /// let input = "let x = 42";
    /// let mut state = LexerState::new(input, None);
    ///
    /// // ... æ·»åŠ  token ...
    ///
    /// let diagnostics = state.success(TestToken::Eof);
    /// match diagnostics.result {
    ///     Ok(token_stream) => {
    ///         println!("è¯æ³•åˆ†ææˆåŠŸï¼Œå…± {} ä¸ª token", token_stream.tokens.len());
    ///     }
    ///     Err(error) => {
    ///         println!("è¯æ³•åˆ†æå¤±è´¥: {}", error);
    ///     }
    /// }
    /// ```
    pub fn success(mut self) -> GaiaDiagnostics<TokenStream<'input, T>> {
        let position = SourcePosition { offset: self.input.len(), length: 0, line: self.line, column: self.column };
        self.tokens.push(Token { token_type: T::END_OF_STREAM, position });
        GaiaDiagnostics { result: Ok(TokenStream::new(self.input, self.tokens)), diagnostics: self.diagnostics }
    }

    /// åˆ›å»ºå¤±è´¥çš„è¯Šæ–­ç»“æœ
    ///
    /// åˆ›å»ºåŒ…å«è‡´å‘½é”™è¯¯çš„å¤±è´¥è¯Šæ–­ç»“æœ
    ///
    /// å½“é‡åˆ°æ— æ³•æ¢å¤çš„é”™è¯¯æ—¶ä½¿ç”¨æ­¤æ–¹æ³•ï¼Œå®ƒä¼šè¿”å›ä¸€ä¸ªè‡´å‘½é”™è¯¯ï¼Œ
    /// è¯æ³•åˆ†æè¿‡ç¨‹åº”è¯¥ç«‹å³åœæ­¢ã€‚
    ///
    /// # å‚æ•°
    ///
    /// * `fatal` - è‡´å‘½é”™è¯¯
    ///
    /// # è¿”å›å€¼
    ///
    /// è¿”å› `GaiaDiagnostics<TokenStream<T>>` å®ä¾‹ï¼Œå…¶ä¸­ï¼š
    /// * `result` æ˜¯ `Err(fatal)`
    /// * `diagnostics` åŒ…å«ä¹‹å‰æ”¶é›†çš„æ‰€æœ‰è¯Šæ–­ä¿¡æ¯
    ///
    /// # ç¤ºä¾‹
    ///
    /// ```rust
    /// # use gaia_types::lexer::{LexerState, TokenType};
    /// # use gaia_types::errors::{GaiaError, SourceLocation};
    /// # #[derive(Clone, Copy, Debug, PartialEq)] enum TestToken { Eof }
    /// # impl TokenType for TestToken { const EOF: Self = TestToken::Eof; }
    /// let input = "let x = @";
    /// let mut state = LexerState::new(input, None);
    ///
    /// // ... å¤„ç†è¾“å…¥ ...
    ///
    /// // é‡åˆ°è‡´å‘½é”™è¯¯
    /// let fatal_error = GaiaError::syntax_error("æ— æ³•è¯†åˆ«çš„å­—ç¬¦",
    ///     SourceLocation { line: 1, column: 9, url: None });
    /// let diagnostics = state.failure(fatal_error);
    ///
    /// assert!(diagnostics.result.is_err());
    /// ```
    pub fn failure(self, fatal: GaiaError) -> GaiaDiagnostics<TokenStream<'input, T>> {
        GaiaDiagnostics { result: Err(fatal), diagnostics: self.diagnostics }
    }
}
